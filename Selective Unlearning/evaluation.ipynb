{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "model_path = \"/scratch/mk8475/WHPFT3\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype='auto'\n",
    ").eval()\n",
    "\n",
    "# Prompt content: \"hi\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"hi\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_prompts.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "responses = []\n",
    "for item in data:\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": item}\n",
    "    ]\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')\n",
    "    output_ids = model.generate(input_ids.to('cuda'))\n",
    "    response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    responses.append({'prompt': prompt, 'response': response})\n",
    "\n",
    "# Output the responses to a JSON file\n",
    "with open('model_responses.json', 'w') as outfile:\n",
    "    json.dump(responses, outfile, indent=4)\n",
    "\n",
    "print(\"Responses have been saved to 'model_responses.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
