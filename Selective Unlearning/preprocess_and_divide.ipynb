{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "huggingface_token = ''\n",
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = str(huggingface_token)"
      ],
      "metadata": {
        "id": "1M7axB2XpadQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "5r7gAooEoDKd",
        "outputId": "87d77fe5-0611-4c7a-e6c7-b407deb52947"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Book1.pdf\n",
            "/content/Book2.pdf\n",
            "/content/Book3.pdf\n",
            "/content/Book4.pdf\n",
            "/content/Book5.pdf\n",
            "/content/Book6.pdf\n",
            "/content/Book7.pdf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Books.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import os\n",
        "from transformers import LlamaTokenizer\n",
        "\n",
        "baseline_model_path = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "# Initialize the Llama tokenizer\n",
        "tokenizer = LlamaTokenizer.from_pretrained(baseline_model_path)\n",
        "\n",
        "def tokenize_and_chunk(text, chunk_size=512):\n",
        "    # Tokenize the text\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    # Break tokens into chunks of 'chunk_size', ensuring not to split words\n",
        "    chunks = [tokens[i:i + chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
        "\n",
        "    # Convert token ids back to strings and ensure the last word is not split\n",
        "    chunk_texts = []\n",
        "    for chunk in chunks:\n",
        "        chunk_text = tokenizer.decode(chunk, clean_up_tokenization_spaces=True)\n",
        "        chunk_texts.append(chunk_text)\n",
        "    return chunk_texts\n",
        "\n",
        "# Function to read PDF and tokenize text\n",
        "def process_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return tokenize_and_chunk(text)\n",
        "\n",
        "# Paths to your PDF files\n",
        "pdf_paths = [\"/content/Book1.pdf\", \"/content/Book2.pdf\", \"/content/Book3.pdf\",\n",
        "             \"/content/Book4.pdf\", \"/content/Book5.pdf\", \"/content/Book6.pdf\",\n",
        "             \"/content/Book7.pdf\"]\n",
        "\n",
        "# Tokenize all PDFs and collect texts\n",
        "all_texts = []\n",
        "for path in pdf_paths:\n",
        "    print(path)\n",
        "    all_texts.extend(process_pdf(path))\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(all_texts, columns=['text'])\n",
        "csv_path = \"Books.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Output the path to the saved CSV file\n",
        "csv_path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "p2aU24ijov-V",
        "outputId": "5293eb2a-ea89-474d-83ee-a3cd7b53a4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'?”\\n      “I’ve come to bring Harry to his aunt and uncle. They’re the only family\\nhe has left now.”\\n      “You don’t mean – you can’t mean the people who live here?” cried\\nProfessor McGonagall, jumping to her feet and pointing at number four.\\n“Dumbledore — you can’t. I’ve been watching them all day. You couldn’t find\\ntwo people who are less like us. And they’ve got this son — I saw him kicking\\nhis mother all the way up the street, screaming for sweets. Harry Potter come\\nand live here!”\\n      “It’s the best place for him,” said Dumbledore firmly. “His aunt and\\nuncle will be able to explain everything to him when he’s older. I’ve written\\nthem a letter.”\\n      “A letter?” repeated Professor McGonagall faintly, sitting back down on\\nthe wall. “Really, Dumbledore, you think you can explain all this in a letter?\\nThese people will never understand him! He’ll be famous — a legend — I\\nwouldn’t be surprised if today was known as Harry Potter day in the future —\\nthere will be books written about Harry — every child in our world will know\\nhis name!”\\n      “Exactly.” said Dumbledore, looking very seriously over the top of his\\nhalf-moon glasses. “It would be enough to turn any boy’s head. Famous before\\nhe can walk and talk! Famous for something he won’t even remember! Can you\\nsee how much better off he’ll be, growing up away from all that until he’s ready\\nto take it?”\\n      Professor McGonagall opened her mouth, changed her mind, swallowed,\\nand then said, “Yes — yes, you’re right, of course. But how is the boy getting\\nhere, Dumbledore?” She eyed his cloak suddenly as though she thought he might\\nbe hiding Harry underneath it.\\n      “Hagrid’s bringing him.”\\n      “You think it — wise — to trust Hagrid with something as important as\\nthis?”\\n      “I would trust Hagrid with my life,” said Dumbledore.\\n     '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "from transformers import LlamaTokenizer\n",
        "\n",
        "# Initialize the Llama tokenizer with a hypothetical path; replace with your actual path\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Preprocess the text to handle hyphens and line breaks\n",
        "    text = text.replace('-\\n', '').replace('\\n', ' ')\n",
        "    return text\n",
        "\n",
        "def tokenize_and_chunk(text):\n",
        "    # Preprocess the text\n",
        "    preprocessed_text = preprocess_text(text)\n",
        "\n",
        "    # Tokenize the text and get token ids\n",
        "    tokens = tokenizer(preprocessed_text, add_special_tokens=False, return_tensors='pt')['input_ids'].squeeze()\n",
        "\n",
        "    # Initialize an empty list to store chunks\n",
        "    chunks = []\n",
        "    for i in range(0, tokens.size(0), 512):\n",
        "        # Ensure the chunk does not exceed 512 tokens\n",
        "        chunk = tokens[i:i+512]\n",
        "        # Decode the chunk back to text\n",
        "        chunk_text = tokenizer.decode(chunk, clean_up_tokenization_spaces=True)\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def process_pdf(file_path):\n",
        "    doc = fitz.open(file_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return tokenize_and_chunk(text)\n",
        "\n",
        "# Assume paths to PDF files are correctly set\n",
        "pdf_paths = [\"/content/Book1.pdf\", \"/content/Book2.pdf\", \"/content/Book3.pdf\",\n",
        "             \"/content/Book4.pdf\", \"/content/Book5.pdf\", \"/content/Book6.pdf\",\n",
        "             \"/content/Book7.pdf\"]\n",
        "\n",
        "all_texts = []\n",
        "for path in pdf_paths:\n",
        "    print(path)\n",
        "    all_texts.extend(process_pdf(path))\n",
        "\n",
        "# Save tokenized and chunked text to a DataFrame and then to a CSV file\n",
        "df = pd.DataFrame(all_texts, columns=['text'])\n",
        "csv_path = \"Books.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Saved tokenized texts to {csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pFJpjrDozIj",
        "outputId": "efffd1e8-158d-4c56-a940-dcde4b9a7d2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Book1.pdf\n",
            "/content/Book2.pdf\n",
            "/content/Book3.pdf\n",
            "/content/Book4.pdf\n",
            "/content/Book5.pdf\n",
            "/content/Book6.pdf\n",
            "/content/Book7.pdf\n",
            "Saved tokenized texts to Books.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer(df['text'][12], add_special_tokens=False, return_tensors='pt')['input_ids'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJxvD66qtPYP",
        "outputId": "45243602-cbc4-435e-81de-151dcb1228d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3TQWAjEt5Ce",
        "outputId": "c3ec4eb4-8043-439a-fe51-a8fc8f11d42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3583"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g8ViyzIAt88n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}